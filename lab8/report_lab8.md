运行的代码，有两个一个是用于测试的，

```
make grade
```

还有一个用于正常操作的，新增了一个disk0

```
qemu-system-i386 -s -hda bin/ucore.img -drive file=./bin/swap.img,media=disk,cache=writeback -drive file=./bin/sfs.img,media=disk,cache=writeback -monitor vc --nographic
```

这里面参考一下下Makefile，就是说，需要多加一个disk了，作为文件系统。容我看看Makefile。

他大概就是多加了一个disk，写成上面那个就很舒服了。能用，但是恐怕得去看看那个driver的代码了。

emmm相关理解部分我写在KeyPoints里面了。至于跑的时候报了一个什么乱七八糟的pagefault的错误，我估摸着，应该是，那个load_icode的部分，那整个直接给空了，没给人新的进程创建memory，可不能不出错啊。

# 练习0：填写已有实验

本实验依赖实验1/2/3/4/5/6/7。请把你做的实验1/2/3/4/5/6/7的代码填入本实验中代码中有“LAB1”/“LAB2”/“LAB3”/“LAB4”/“LAB5”/“LAB6” /“LAB7”的注释相应部分。并确保编译通过。注意：为了能够正确执行lab8的测试应用程序，可能需对已完成的实验1/2/3/4/5/6/7的代码进行进一步改进。

#### answers：
反正写完了跑不了，就酱紫，能说什么。

# 练习1: 完成读文件操作的实现（需要编码）
首先了解打开文件的处理流程，然后参考本实验后续的文件读写操作的过程分析，编写在sfs_inode.c中sfs_io_nolock读文件中数据的实现代码。

请在实验报告中给出设计实现”UNIX的PIPE机制“的概要设方案，鼓励给出详细设计方案

#### answers：
这部分对应了读文件这件事情。

看了大半天，原来超级简单
首先考虑读取的offset和endpos都在一个块内的情况
如果不是，那么从开头读取不到一个4k的，然后循环找，然后再找结尾——这里面需要考虑endpos和offset在一个块内的情况，也就是读取的长度==0的时候，这个时候，就不要读取了。
我第一次写的代码其实和答案有一定差别，但是，其实意思差不多。无非是实现的方式，以及需要注意的细节部分。毕竟那个blkoff定义在那么前面还没个注释，然后alen是啥我也不清楚，真。。。让人难过。

#### UNIX的PIPE机制

（吐槽一句，这最后一个代码真是越写越简单了，都不给什么参考，我弄个屁啊。）我自己找的参考材料。

```
1、管道（pipe）

管道是进程间通信的主要手段之一。一个管道实际上就是个只存在于内存中的文件，对这个文件的操作要通过两个已经打开文件进行，它们分别代表管道的两端。管道是一种特殊的文件，它不属于某一种文件系统，而是一种独立的文件系统，有其自己的数据结构。根据管道的适用范围将其分为：无名管道和命名管道。

●     无名管道

主要用于父进程与子进程之间，或者两个兄弟进程之间。在linux系统中可以通过系统调用建立起一个单向的通信管道，且这种关系只能由父进程来建立。因此，每个管道都是单向的，当需要双向通信时就需要建立起两个管道。管道两端的进程均将该管道看做一个文件，一个进程负责往管道中写内容，而另一个从管道中读取。这种传输遵循“先入先出”（FIFO）的规则。

●     命名管道

命名管道是为了解决无名管道只能用于近亲进程之间通信的缺陷而设计的。命名管道是建立在实际的磁盘介质或文件系统（而不是只存在于内存中）上有自己名字的文件，任何进程可以在任何时间通过文件名或路径名与该文件建立联系。为了实现命名管道，引入了一种新的文件类型——FIFO文件（遵循先进先出的原则）。实现一个命名管道实际上就是实现一个FIFO文件。命名管道一旦建立，之后它的读、写以及关闭操作都与普通管道完全相同。虽然FIFO文件的inode节点在磁盘上，但是仅是一个节点而已，文件的数据还是存在于内存缓冲页面中，和普通管道相同。
```

简单来说，反正应该是在虚拟内存中的一个文件。

首先需要考虑的是两个进程之间的地址空间，是不能互相访问的，这里需要能够被看见，如果放在内核空间，我想这不是一个优秀的设计，对吧。

所以为了能够访问到到，需要设计一个专有的数据结构来表示——但这可以依托文件系统。不过一个是加读写锁，另一个是pipe是需要考虑到引用的进程的上限的，因为是个单向管道嘛，两边各一个。

相关操作的话，文件inode结构对应的一些操作改一改就完成了——但专门设计一个也不是不可以的。可是他不应当放在文件系统内部，而是放在两个proc的结构里面，并且是一个链表的形式，因为说不好啊，每个程序，到底有几个管道嘛。

关于具体的存储位置，正如前面所说，应当放在用户空间的——不然大家都申请管道，那内核空间哪里够啊，那么要使得另一个用户也能够访问到，其实上面的设计就够了。如果是无名管道的话，在父进程的用户空间开辟一个区域（比如说4k）fork的时候，直接将相关内容给子进程就完了。


# 练习2: 完成基于文件系统的执行程序机制的实现（需要编码）
改写proc.c中的load_icode函数和其他相关函数，实现基于文件系统的执行程序机制。执行：make qemu。如果能看看到sh用户程序的执行界面，则基本成功了。如果在sh用户界面上可以执行”ls”,”hello”等其他放置在sfs文件系统中的其他执行程序，则可以认为本实验基本成功。

请在实验报告中给出设计实现基于”硬链接和软链接机制“的概要设方案，鼓励给出详细设计方案

#### answers：
这应该对应一各加载器的部分。
我觉得首先得看看之前的load_icode是怎么实现的。那玩意老长，而且感觉很多代码可以照抄，毕竟，怎么说呢，内存分配部分其实差不多。
有区别的是，读取文件的部分，一个是内存中直接拷贝，见之前的user的实验，而现在则是需要从文件读取。基于fd这个文件句柄。

这里面需要记录一下file这个结构
```
struct file {
    enum {
        FD_NONE, FD_INIT, FD_OPENED, FD_CLOSED,
    } status;
    bool readable;
    bool writable;
    int fd;
    off_t pos;
    struct inode *node;
    int open_count;
};
```
所得到的的文件句柄是这个结构里面的fd，所以需要从这个结构得到struct inode

另外，实验出于测试目的，还写了sh.c文件，相当于一个linux的shell。proc.c里面也有很多其他地方的更改，有兴趣可以进行和lab7的对比。
#### load_icode
这边送了一个函数调用load_icode_read，免去了一大堆的读写麻烦。——但是并不是很明白这个buff该怎么处理，是说一口气读进来整个文件然后按照这个buff的内容再搞事情呢（因为反正文件也不是很大对吧）。还是说只把开头的部分读取进来，然后根据需要再进行读取呢？如果只考虑空间，那肯定后者好，因为这里面需要为了buff再开辟一个空间，反之，前者会减少读取磁盘的次数，因为一整个读进来就好了，后者还要考虑一大堆东西，我个人倾向于后者。没了。

不得不说，这块内存的处理（尤其是那几个函数的乱七八糟的变量，还没得说明emmm尽管看意思就能明白但是，毕竟太多了不是嘛）是我遇到的第一个难题。
ok下面的一些工作准备好之后，基本上就是把lab7里面的东西复制过来即可，有区别的地方在于一个elf和prohdr的读取和内存拷贝的变成了文件读取。

#### argv和argc

之前的工作其实没有考虑这个问题，直到我发现如果不管这个事情，会报那个vma缺少栈顶地址的错误。因为参数是压栈的方式传给子函数的，所以应该是这个问题。emmm今天不早了，我先把代码传了，剩下的解读，就明天再说吧。

我找到了个参考的html：

https://www.cnblogs.com/wuhualong/p/ucore_lab8_report.html

简而言之，就是说，要访问事先说好的argc，然后说，栈顶地址减少一些，虽然不会读取argv，但是还是能运行的。

我看了一下下，在用户的代码里面，就是initcode.s里面增加了几条，从而导致了这个访问sp栈顶的错误。
```
	movl (%esp), %ebx
    lea 0x4(%esp), %ecx
```
这里必须说起main函数argv和argc的压栈

#### sh函数的argv

我先去找了这个程序的参数——这总是可以的吧，定义应该是这个样子的
```
const char *argv[] = {path, ##__VA_ARGS__, NULL};
```
再继续往下

sys_exec中是这么定义的
```
sys_exec(uint32_t arg[]) {
    const char *name = (const char *)arg[0];
    int argc = (int)arg[1];
    const char **argv = (const char **)arg[2];
    return do_execve(name, argc, argv);
}
```
也就是第一个参数是程序名称，第二个参数是参数的数量，后面的则是其他参数。

考虑到对应的参数kernel_execve中的内容
```
static int
kernel_execve(const char *name, const char **argv) {
    int argc = 0, ret;
    while (argv[argc] != NULL) {
        argc ++;
    }
    asm volatile (
        "int %1;"
        : "=a" (ret)
        : "i" (T_SYSCALL), "0" (SYS_exec), "d" (name), "c" (argc), "b" (argv)
        : "memory");
    return ret;
}
```
这里面前两个输入参数请忽略，那个是中断号巴拉巴拉的东西。但是这里面，的argc并不是只是后面的argv，还包括了之前的name，不然在do_execve中有一行
```
if (!(argc >= 1 && argc <= EXEC_MAX_ARG_NUM)) {
        return -E_INVAL;
    }
```
说明起码包含一个，也就是说，这里面argc的数量大于等于1，所以一定包含了name的。而argc则是插入在name和后面的argv之间的

随后用了一个copy_kargv的函数，将argv拷贝到kargv里面。

#### 在load_icode里面的argc和argv
这里面首先计算kargv总共占用了多少个long的长度，然后将这些参数复制到那个栈顶空间去，可用的栈顶空间stacktop则USTACKTOP减去这些长度。

另外，为了能够分割不同的argv，还多加了个argc个char，最后栈顶的地址，是USTACKTOP-argv的内容在减去为了分割加入的\0

#### proc结构初始化里面的工作
好吧在这之前，还要弄一下proc里面，alloc一个proc结构时候的工作。就是需要初始化里面的fs的结构，感觉根本就没找到那个fs就gg了，我再去看看。但是idleproc没毛病啊，这里面用了一个idleproc->filesp = files_create()
但是貌似这并没有什么卵用

#### fork里面增加的部分
找了一圈，原来fork也要增加。
只是增加一个copy_files即可

#### 硬链接和软链接机制

我直接找了个参考资料，感觉这个实现机制没有那么麻烦，只不过是加一点点东西罢了。

```
观察到保存在磁盘上的inode信息均存在一个nlinks变量用于表示当前文件的被链接的计数，因而支持实现硬链接和软链接机制；
如果在磁盘上创建一个文件A的软链接B，那么将B当成正常的文件创建inode，然后将TYPE域设置为链接，然后使用剩余的域中的一个，指向A的inode位置，然后再额外使用一个位来标记当前的链接是软链接还是硬链接；
当访问到文件B（read，write等系统调用），判断如果B是一个链接，则实际是将对B指向的文件A（已经知道了A的inode位置）进行操作；
当删除一个软链接B的时候，直接将其在磁盘上的inode删掉即可；
如果在磁盘上的文件A创建一个硬链接B，那么在按照软链接的方法创建完B之后，还需要将A中的被链接的计数加1；
访问硬链接的方式与访问软链接是一致的；
当删除一个硬链接B的时候，除了需要删除掉B的inode之外，还需要将B指向的文件A的被链接计数减1，如果减到了0，则需要将A删除掉；
```

软连接就相当于一个指针，硬链接就相当于将文件copy一份，只不过，两份完全相同，所以就直接拷贝一个名字。

```
硬链接是有着相同 inode 号仅文件名不同的文件，因此硬链接存在以下几点特性：

文件有相同的 inode 及 data block；
只能对已存在的文件进行创建；
不能交叉文件系统进行硬链接的创建；
不能对目录进行创建，只可对文件创建；
删除一个硬链接文件并不影响其他有相同 inode 号的文件。
```

