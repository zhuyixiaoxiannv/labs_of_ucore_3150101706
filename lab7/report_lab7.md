开局依然碰见报错，心塞塞，不知道是不是代码复制过去的问题，报一个general protection的错误，

运行的代码，有两个一个是用于测试的，

```
make grade
```

还有一个用于正常操作的，同lab4

```
qemu-system-i386 -s -hda bin/ucore.img -drive file=./bin/swap.img,media=disk,cache=writeback -monitor vc --nographic
```

我表示一脸懵逼，如果说前面内核态还能像个程序一样调试的话，现在这个用户态就很。。。

报的错是general protection，然后呢，做的事情是访问系统的TSS段。。。？这不是显然会出错吗？

emmmm于是我去找了lab6，没错啊，lab6改成运行badsegment也会有这个问题啊，那就不是代码的问题，对吧好好地哈哈哈哈。那为啥lab6的make grade就没事啊。哦，可能是，这个lab7的代码跑起来有点慢。。。因为时钟的问题。算了等着吧。慢慢先看看这个project让人做什么。我去写KeyPoints了。

哦，那没事了，就一件事，这个代码跑起来有点慢。手动狗头。

# 练习0：填写已有实验

本实验依赖实验1/2/3/4/5/6。请把你做的实验1/2/3/4/5/6的代码填入本实验中代码中有“LAB1”/“LAB2”/“LAB3”/“LAB4”/“LAB5”/“LAB6”的注释相应部分。并确保编译通过。注意：为了能够正确执行lab7的测试应用程序，可能需对已完成的实验1/2/3/4/5/6的代码进行进一步改进。

#### answers:
把代码拷贝过去就完了

# 练习1: 理解内核级信号量的实现和基于内核级信号量的哲学家就餐问题（不需要编码）

完成练习0后，建议大家比较一下（可用kdiff3等文件比较软件）个人完成的lab6和练习0完成后的刚修改的lab7之间的区别，分析了解lab7采用信号量的执行过程。执行make grade，大部分测试用例应该通过。

请在实验报告中给出内核级信号量的设计描述，并说其大致执行流流程。

请在实验报告中给出给用户态进程/线程提供信号量机制的设计方案，并比较说明给内核级提供信号量机制的异同。

#### Answers：
除了lab6里面的优先级的部分，其他都能过

#### 信号量：

```
typedef struct {
    int value;                           //信号量的当前值
    wait_queue_t wait_queue;     //信号量对应的等待队列
} semaphore_t;
```

数据结构的描述如上面所述，大概就是当前信号量的值，和对应的等待队列，等待队列要看那个底层支撑，就是一个出入队列的问题，不是很关键。

PV操作没有那么麻烦，就是操作系统里面学到的基础，这里面用一个down函数和up函数

注意一段描述：

```
value>0，表示共享资源的空闲数
vlaue<0，表示该信号量的等待队列里的进程数
value=0，表示等待队列为空
```

这里面value==0的地方，共享资源已经没了，但是也没有新的进程申请共享资源。

#### 哲学家进餐的信号量实现的过程

每个哲学家用一个philosopher_using_semaphore的内核进程来表示，这里面调用了两个函数，一个是拿叉子，一个是放回叉子，而在拿叉子和放回的叉子的过程中调用了信号量，让叉子不能被别的进程所访问。

在拿叉子和取叉子当中的信号量，则是由sem.h里面定义的一个down和一个up函数实现，注意一下下这里面的monitor则是管程的文件，跟这个没啥太大关系

而这个down和up里面则主要是对这个信号量的管理，如果有能使用的资源，那么就减减，然后返回，否则就置位wait然后让他schedule，schedule完了之后还跑到这行，就继续跑。

这里面的proc的状态并不是runnable，只缺处理机。而是缺少这个信号量，属于阻塞态的。

另外就是一个wait这个queue的处理操作。用了一个wait_t的结构体。

（说来复杂，其实看看代码就完了，讲真我觉得懂了PV操作之后，加个队列，也就这样的内容了。

#### 请在实验报告中给出给用户态进程/线程提供信号量机制的设计方案，并比较说明给内核级提供信号量机制的异同。

对于用户态进程/线程想要使用信号量机制，基本就是要在trap里的systemcall里面加东西了， 操作系统内核直接调用就完事了。

信号量主要包括一个value和相应队列，以及相关的PV操作，PV操作显然是需要分别弄一个systemcall的，另外还需要有创建申请信号量以及撤销一个信号量的操作。

# 练习2: 完成内核级条件变量和基于内核级条件变量的哲学家就餐问题（需要编码）

首先掌握管程机制，然后基于信号量实现完成条件变量实现，然后用管程机制实现哲学家就餐问题的解决方案（基于条件变量）。

执行：make grade 。如果所显示的应用程序检测都输出ok，则基本正确。如果只是某程序过不去，比如matrix.c，则可执行 make run-matrix 命令来单独调试它。大致执行结果可看附录。（使用的是**qemu-1.0.1**）。

请在实验报告中给出内核级条件变量的设计描述，并说其大致执行流流程。

请在实验报告中给出给用户态进程/线程提供条件变量机制的设计方案，并比较说明给内核级提供条件变量机制的异同。

感觉要我做的事情其实很少，要我填写的代码，check_sync里面基本上和使用信号量的东西差不多，然后monitor里面基本上都给出来了。并不明白有什么要加的。（不过过会和答案对照一下下吧）
u1s1，这个感觉make grade之后，并不能充分判断这个写的对不对啊。一脸迷茫。
哦，我这边跑不下去了，基本就是说，好像确实有些不对劲的地方。反正跑着跑着凉了就是跑的不对是吧啊哈哈哈。
哔了狗了，真的跑完了。我自己都没读懂这管程讲的啥玩意儿，就照着写，然后就完了，这就是传说中的thu，负重前行的永远都是老师和助教嘛。。。笑哭了我。

顺带看了一下下lab7_result的代码，基本差不多，但是这边，有一个，cond_wait的函数忘记了，大概，这么说吧，不然就根本没有用到这个函数的地方了。要注意对称性。其实不是对不对称的事情，而是这种变量，没有被阻塞的考虑。so。。。洗洗睡吧。

另外还有就是sleep里面缺少一个正确性的东西，我不知道怎么回事。小老弟你怎么回事。

至于lab7_results，小老弟，这简直不行啊，直接跑都能出点毛病。。。小老弟，你太惨了。不过我觉得不是什么大问题倒是真的。至于那个sleep是什么鬼，为什么怎么弄都要跑那么久

（顺带一说，上次清理磁盘，直接删了git。。。真的mmp）

emmm对于那个lab7里面priority的错误是，因为lab6里面的进程调度优先度的问题，那么在sleep里面的问题，我把check_sycn注释掉不运行之后，依然存在，说明跟本实验的关系不是很大。看了具体代码之后我个人的猜想是，就是，由于sleep里面涉及到对于时间的读取，而在模拟器中的时间读取其实前面也一直报错误，所以可能跟这个有关系。

他说

```
use 99999 msecs.
```

大概。。。是我太菜了吧，我怎么怎么觉得这像是超时了的样子。不应该啊。

我查看了一下下，开始时间是1，结束时间是

```
time is:00100000.
```

而且跑出来整整齐齐，所以我觉得是，grade检测方式的问题，估计是这个写错了，那，不管了。

明天写一下管程这个的理解（我感觉看的贼晕乎乎的）


# Linux 的 RCU

RCU的做法是，对于读者，基本没有什么需要取得锁的事情——但是读这个过程需要禁止中断，出于一致性的考虑，对于写者，首先通过读给他copy一个副本，然后在这个副本上面写，并寻求一个合适的时间写回去。这个写回去是调用一个回调函数注册来实现的，多个写者肯定会注册多个回调函数，并且时间有先后。

```
读者在访问被RCU保护的共享数据期间不能被阻塞，这是RCU机制得以实现的一个基本前提，也就说当读者在引用被RCU保护的共享数据期间，读者所在的CPU不能发生上下文切换。

写者在访问被RCU保护的共享数据时不需要和读者竞争任何锁，只有在有多于一个写者的情况下需要获得某种锁以与其他写者同步。写者修改数据前首先拷贝一个被修改元素的副本，然后在副本上进行修改，修改完毕后它向垃圾回收器注册一个回调函数以便在适当的时机执行真正的修改操作。等待适当时机的这一时期称为grace period，而CPU发生了上下文切换称为经历一个quiescent state，grace period就是所有CPU都经历一次quiescent state所需要的等待的时间。垃圾收集器就是在grace period之后调用写者注册的回调函数来完成真正的数据修改或数据释放操作的。
```

```
以下以链表元素删除为例详细说明这一过程。

写者要从链表中删除元素 B，它首先遍历该链表得到指向元素 B 的指针，然后修改元素 B 的前一个元素的 next 指针指向元素 B 的 next 指针指向的元素C，修改元素 B 的 next 指针指向的元素 C 的 prep 指针指向元素 B 的 prep指针指向的元素 A,在这期间可能有读者访问该链表，修改指针指向的操作是原子的，所以不需要同步，而元素 B 的指针并没有去修改，因为读者可能正在使用 B 元素来得到下一个或前一个元素。写者完成这些操作后注册一个回调函数以便在 grace period 之后删除元素 B，然后就认为已经完成删除操作。垃圾收集器在检测到所有的CPU不在引用该链表后，即所有的 CPU 已经经历了 quiescent state,grace period 已经过去后，就调用刚才写者注册的回调函数删除了元素 B。
```

考虑多个读者的情况，因为不涉及到对原始文件的更改，也不需要锁，所以性能没有损失。

考虑多个写者的情况，这个协调是基于写者之间的同步机制的，这个机制和RCU不是一回事，但总体而言RCU保证写的原子性，就是不会出现，前半部分是写者a的内容，后半部分是写者b的内容的情况。

考虑读者和写者的情况，先读的是初始的，然后写，会更改实际的内容，但是并不会让在写者之前读得到的内容被更改，而之后读的和写者写进去的内容一致。

这里是考虑多个cpu的情况的，所以quiescent state是单个cpu，而所有CPU都不再访问这个链表之后，就完成了grace period。

对于读者的函数操作，就是**rcu_read_lock**()和**rcu_read_unlock**()，其实相当于开关中断，反正就是不允许抢占这段代码。

而写者首先需要跟其他写者竞争，可以用spinlock或者其他方式进行同步，另外，写者也需要和读者进行同步，正如前面所说的，得允许所有的cpu的上下文切换完成后，才能够更改，那么就是用一个**synchronize_rcu**()

他的作用是，在调用这个函数之前就已经申请了rcu_read_lock的进程能够继续使用旧的数据，但是在进入这个函数之后的，就免了。

**call_rcu**()

对于内核级任务而言，如果需要等待writer全都读完，很有可能太晚了，所以这边可以用call_rcu，来注册回调函数，rcu_callback_t func。先去做别的事情，等到所有的reader都退出临界区之后再继续执行回调函数。

**rcu_assign_pointer**()

写的时候需要生成新的数据，前面说delete的时候，等待所有读者，而这边，在写者的数据准备好之后，需要让读者看见，也就是在删除了上面的数据之后，把新的数据让读者看见。所以把旧的指针指向新的。

**rcu_dereference**()

对于读者而言，需要读取的是，拷贝的副本，所以使用的也是一个临时指针，

```
rcu_read_lock();
p1 = rcu_dereference(p);
rcu_read_unlock();
```

并且只能在临界区内使用。